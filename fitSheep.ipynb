{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from math import *\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "myFmt = mdates.DateFormatter('%Hh')\n",
    "\n",
    "import pandas as pd\n",
    "np.set_printoptions(suppress=True,precision=6)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "plt.style.use('ggplot') \n",
    "plt.style.use('seaborn-paper') \n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "import sys\n",
    "\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#from move_ns import moveNS\n",
    "from move_ns_whiten import moveNS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and display individual time series lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up lower level GP locations covering 24 hours\n",
    "Z = np.linspace(0,24,num=25,endpoint=False).astype(np.float64)[:,None]\n",
    "#np.random.shuffle(Z)\n",
    "#Z = np.random.uniform(0,24,size=(25,1)).astype(np.float64)\n",
    "#Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_data(skip_i=1,skip=3):\n",
    "    \n",
    "    df = pd.read_csv('data/ovejas.csv')\n",
    "\n",
    "    df = df[df.id!=34]\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    df['ID'] = df['id'].astype('category').cat.rename_categories(range(0, df['id'].nunique())).astype('int')\n",
    "    df = df[df['ID']%skip_i==0]\n",
    "    \n",
    "    ID = df['ID'].values \n",
    "    \n",
    "    \n",
    "    \n",
    "    Xgps = df[['lat','lon']].values\n",
    "    minX = np.min(Xgps[:,0])\n",
    "    minY = np.min(Xgps[:,1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    secs =(pd.to_datetime(df['time'])- pd.datetime(2018,1,1)).dt.seconds.astype(float).values\n",
    "    days = (pd.to_datetime(df['time'])- pd.datetime(2018,1,1)).dt.days.astype(float).values\n",
    "    T = (days*24*60+secs/60)/(60*24) #days\n",
    "    T = T-np.min(T)\n",
    "\n",
    "    rescale = 24  # use hours to improve numerical stability\n",
    "    T = T * rescale\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # use geodesic to get the straight line distance between two points\n",
    "    Xmetres = np.array([geodesic((xloc,minY), (minX,minY)).meters for xloc in Xgps[:,0]])\n",
    "    Ymetres = np.array([geodesic((minX,yloc), (minX,minY)).meters for yloc in Xgps[:,1]])\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    X = np.array([Xmetres, Ymetres]).T\n",
    "    \n",
    "    T=T[::skip,None]\n",
    "    X=X[::skip]\n",
    "    ID=IDs[::skip]\n",
    "\n",
    "    return X, T, ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-3f542ce23d13>:27: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  secs =(pd.to_datetime(df['time'])- pd.datetime(2018,1,1)).dt.seconds.astype(float).values\n",
      "<ipython-input-3-3f542ce23d13>:28: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  days = (pd.to_datetime(df['time'])- pd.datetime(2018,1,1)).dt.days.astype(float).values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 0\n",
      "processing 1\n",
      "processing 2\n",
      "processing 3\n",
      "processing 4\n",
      "processing 5\n",
      "processing 6\n",
      "processing 7\n",
      "processing 8\n",
      "processing 9\n",
      "processing 10\n",
      "processing 11\n",
      "processing 12\n",
      "processing 13\n",
      "processing 14\n",
      "processing 15\n",
      "processing 16\n",
      "processing 17\n",
      "processing 18\n",
      "processing 19\n",
      "processing 20\n",
      "processing 21\n",
      "processing 22\n",
      "processing 23\n",
      "processing 24\n",
      "processing 25\n",
      "processing 26\n",
      "(225953, 2)\n"
     ]
    }
   ],
   "source": [
    "X,T,ID = setup_data(skip_i=1,skip=2)\n",
    "X[:,0] = X[:,0]-X[:,0].mean()\n",
    "X[:,1] = X[:,1]-X[:,1].mean()\n",
    "X[:,0] = X[:,0]/1000#X[:,1].std()\n",
    "X[:,1] = X[:,1]/1000#X[:,1].std()\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_shift(x):\n",
    "    # softplus transform with shift \n",
    "    return tf.nn.softplus(x)+1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up the non-stationary GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def periodic_kernel(x1,x2):\n",
    "    # periodic kernel with parameter set to encode\n",
    "    # daily activity pattern (period=rescale).\n",
    "    x2a = x2+ np.float64(1)#2*0.18)#24/pi)\n",
    "    return tfp.math.psd_kernels.ExpSinSquared(x1,x2a,np.float64(24.0))\n",
    "\n",
    "# transform for parameter to ensure positive\n",
    "transforms=[sp_shift,sp_shift] \n",
    "#transforms=[sp_shift] \n",
    "\n",
    "# diffuse priors on parameters\n",
    "lpriors = [tfd.Normal(loc = np.float64(0.),scale=np.float64(1)),\n",
    "           tfd.Normal(loc = np.float64(0),scale=np.float64(1)),\n",
    "           tfd.Normal(loc = np.float64(0),scale=np.float64(10.))]\n",
    "           \n",
    "apriors = [tfd.Normal(loc = np.float64(0.),scale=np.float64(1)),\n",
    "           tfd.Normal(loc = np.float64(0.),scale=np.float64(1)),\n",
    "           tfd.Normal(loc = np.float64(0),scale=np.float64(10.))]\n",
    "\n",
    "\n",
    "lparams_init = [0.0,0.0,0.0]#p.mean().numpy() for p in lpriors]\n",
    "aparams_init = [0.0,0.0,0.0]#[0.0 for p in apriors]\n",
    "\n",
    "\n",
    "# create the model #2880\n",
    "mover = moveNS(T,X,Z, ID, BATCH_SIZE=1000, MIN_REMAIN=500,velocity=True, std_obs_noise=100, mean_obs_noise=10,\n",
    "                        akernel=periodic_kernel, \n",
    "                        aparams_init=aparams_init, \n",
    "                        apriors=apriors, \n",
    "                        atransforms=transforms,\n",
    "                        lkernel=periodic_kernel, \n",
    "                        lparams_init=lparams_init, \n",
    "                        lpriors=lpriors, \n",
    "                        ltransforms=transforms)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = tf.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-1,\n",
    "    decay_steps=10,\n",
    "    decay_rate=0.99,\n",
    "    staircase=True)\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,beta_2=0.99)\n",
    "train_steps = 2000\n",
    "pbar = tqdm(range(train_steps))\n",
    "loss_history = np.zeros((train_steps))\n",
    "for i in pbar:\n",
    "    with tf.GradientTape() as t:\n",
    "        loss = -mover.log_posterior(*mover.kernel_params)\n",
    "    loss_history[i] = loss.numpy()\n",
    "    pbar.set_description(\"Loss %f\" % loss_history[i])\n",
    "\n",
    "    gradients = t.gradient(loss, mover.kernel_params)\n",
    "    optimizer.apply_gradients(zip(gradients, mover.kernel_params))\n",
    "#n=3.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history[1500:])\n",
    "plt.show()\n",
    "loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = mover.get_lengthscale()\n",
    "plt.plot(Z,lengths,'o',c='b')\n",
    "Zin = np.linspace(0,24,num=500,endpoint=False).astype(np.float64)[:,None]\n",
    "\n",
    "lengths = mover.get_lengthscale(X=Zin)\n",
    "plt.plot(Zin,lengths)\n",
    "plt.show()\n",
    "\n",
    "amps = mover.get_amplitude()\n",
    "plt.plot(Z,amps,'o',c='b')\n",
    "Zin = np.linspace(0,24,num=500,endpoint=False).astype(np.float64)[:,None]\n",
    "amps = mover.get_amplitude(X=Zin)\n",
    "plt.plot(Zin,amps)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Z,(mover.kernel_params[2]).numpy(),'-o')\n",
    "#plt.ylim(0,0.5)\n",
    "plt.show()\n",
    "plt.plot(Z,(mover.kernel_params[6]).numpy(),'-o')\n",
    "plt.show()\n",
    "#6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "opt_step = []\n",
    "for var in optimizer.variables():\n",
    "    if '/v:' in var.name:\n",
    "        opt_step.append((np.sqrt((var.numpy()))+1e-16)**-1)\n",
    "        \n",
    "        \n",
    "plt.plot(opt_step[2][0:25],'.')\n",
    "plt.plot(opt_step[6][0:25],'.')\n",
    "#plt.ylim(-0.5,1)\n",
    "#plt.show()\n",
    "\n",
    "for o in opt_step:\n",
    "    print(o.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004842485281937078\n",
      "0.012115679242881108\n",
      "[0.005525 0.012081 0.007938 0.028228 0.008822 0.097371 0.010387 0.01392\n",
      " 0.015413 0.011206 0.013048 0.016353 0.018095 0.024215 0.036783 0.054245\n",
      " 0.088079 0.165327 0.339472 0.81622  1.       1.       1.       1.\n",
      " 1.      ]\n",
      "0.03458662682408681\n",
      "0.027518707273064325\n",
      "0.002804763469339627\n",
      "[0.003491 0.006215 0.005549 0.011827 0.006601 0.073222 0.007797 0.01013\n",
      " 0.01003  0.006337 0.006785 0.007145 0.006939 0.008356 0.010563 0.01337\n",
      " 0.019494 0.031508 0.055949 0.116227 0.289064 0.868092 1.       1.\n",
      " 1.      ]\n",
      "0.0054500266418088675\n",
      "0.09960409488610739\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(opt_step)):\n",
    "    o=opt_step[i]\n",
    "    if len(o.shape):\n",
    "        o[o>1]=1\n",
    "    opt_step[i]=o#opt_step[i]*0+1e-2\n",
    "    print(opt_step[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, param in enumerate(mover.kernel_params):\n",
    "    print(j,(param.numpy()),param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.softplus(-1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params = [i.numpy() for i in  mover.kernel_params]\n",
    "opt_params.append(Z)\n",
    "\n",
    "with open('opt_params.npy', 'wb') as fp:\n",
    "    pickle.dump(opt_params, fp)\n",
    "with open('opt_step.npy', 'wb') as fp:\n",
    "    pickle.dump(opt_step, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open ('opt_params.npy', 'rb') as fp:\n",
    "    opt_params = pickle.load(fp)\n",
    "    opt_obs_noise = opt_params[0]\n",
    "    opt_ls_mean = opt_params[1]\n",
    "    opt_ls_v = opt_params[2]\n",
    "    opt_ls_amp = sp_shift(opt_params[3]).numpy()\n",
    "    opt_ls_ls = sp_shift(opt_params[4]).numpy()\n",
    "    opt_amp_mean = opt_params[5]\n",
    "    opt_amp_v = opt_params[6]\n",
    "    opt_amp_amp = sp_shift(opt_params[7]).numpy()\n",
    "    opt_amp_ls = sp_shift(opt_params[8]).numpy()\n",
    "    \n",
    "\n",
    "    for i in range(len(mover.kernel_params)):\n",
    "         mover.kernel_params[i].assign(opt_params[i])\n",
    "    #opt_step = opt_params[9]\n",
    "    Z = opt_params[9]\n",
    "\n",
    "    \n",
    "with open ('opt_step.npy', 'rb') as fp:\n",
    "    opt_step = pickle.load(fp)\n",
    "#mover.log_posterior(*mover.kernel_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mover.log_posterior(*mover.kernel_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def ls_periodic_kernel():\n",
    "    # periodic kernel with single variable parameter. Other parameters are set \n",
    "    # to encode daily activity pattern (period=rescale).\n",
    "    # 15 minute correlation time\n",
    "    return tfp.math.psd_kernels.ExpSinSquared(np.float64(opt_ls_amp),np.float64(opt_ls_ls)+np.float64(1.0),np.float64(24.0))\n",
    "\n",
    "def amp_periodic_kernel():\n",
    "    # periodic kernel with single variable parameter. Other parameters are set \n",
    "    # to encode daily activity pattern (period=rescale).\n",
    "    # 15 minute correlation time\n",
    "    return tfp.math.psd_kernels.ExpSinSquared(np.float64(opt_amp_amp),np.float64(opt_amp_ls)+np.float64(1.0),np.float64(24.0))\n",
    "\n",
    "\n",
    "# transform for parameter to ensure positive\n",
    "transforms=[] \n",
    "\n",
    "# prior distribution on parameters - changed to 20 \n",
    "lpriors = [tfd.Normal(loc = np.float64(opt_ls_mean),scale=np.float64(0.10))]\n",
    "apriors = [tfd.Normal(loc = np.float64(opt_amp_mean),scale=np.float64(0.10))]\n",
    "\n",
    "# random initial values of mean and kernel amplitude\n",
    "lparams_init = [opt_ls_mean]\n",
    "aparams_init = [opt_amp_mean]\n",
    "\n",
    "# create the model \n",
    "\n",
    "\n",
    "mover_hmc = moveNS(T,X,Z, ID, BATCH_SIZE=1000, MIN_REMAIN= 500, velocity=True, std_obs_noise=0, mean_obs_noise=opt_obs_noise,\n",
    "                        akernel=amp_periodic_kernel, \n",
    "                        aparams_init=aparams_init, \n",
    "                        apriors=apriors, \n",
    "                        atransforms=transforms,\n",
    "                        lkernel=ls_periodic_kernel, \n",
    "                        lparams_init=lparams_init, \n",
    "                        lpriors=lpriors, \n",
    "                        ltransforms=transforms)\n",
    "\n",
    "mover_hmc.kernel_params[1].assign(opt_ls_v)\n",
    "mover_hmc.kernel_params[3].assign(opt_amp_v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3098/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(s2,'-o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sample from the posterior\n",
    "kr = mover.hmc_sample(num_samples=1, skip=0, burn_in=0, num_leapfrog_steps=1, init_step=1e-8)\n",
    "# lengths = mover.get_lengthscale_samples()\n",
    "# amps = mover.get_amplitude_samples()\n",
    "# #  plot the sample for checking\n",
    "# plt.figure(figsize=(10,6))\n",
    "# Z_time=np.array([np.datetime64('2019') + np.timedelta64(int(k*60*60), 's') for k in Z])\n",
    "\n",
    "# for i in range(0,lengths.shape[0]):\n",
    "#     plt.plot(Z_time[np.argsort(Z_time)],lengths[i,np.argsort(Z_time)],c='C1',alpha=0.902,linewidth=1.0)\n",
    "\n",
    "# ax = plt.gca()\n",
    "# ax.xaxis_date()\n",
    "# ax.xaxis.set_major_formatter(myFmt)\n",
    "# plt.figure(figsize=(10,6))\n",
    "\n",
    "# for i in range(0,amps.shape[0]):\n",
    "#     plt.plot(Z_time[np.argsort(Z_time)],amps[i,np.argsort(Z_time)],c='C1', alpha=0.902,linewidth=1.0)\n",
    "# ax = plt.gca()\n",
    "# ax.xaxis_date()\n",
    "# ax.xaxis.set_major_formatter(myFmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ss = np.float64(1e-2)\n",
    "steps = []\n",
    "for j, param in enumerate(mover.kernel_params):\n",
    "    steps.append(ss)\n",
    "\n",
    "# steps = []\n",
    "# for j, param in enumerate(mover.kernel_params):\n",
    "#     if j==2:\n",
    "#         steps.append(ss)\n",
    "#     elif j==6:\n",
    "#         steps.append(ss)\n",
    "#     else:\n",
    "#         steps.append(0)\n",
    "start = time.time()\n",
    "\n",
    "num_samples=10#500\n",
    "burn=10\n",
    "start = time.time()\n",
    "mover.num_samples = num_samples\n",
    "#kr = mover.nuts_sample(num_samples=num_samples, skip=0, burn_in=burn, init_step=steps)\n",
    "\n",
    "end = time.time()\n",
    "print(i,end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_kernel = tfp.mcmc.NoUTurnSampler(target_log_prob_fn=mover.log_posterior,max_tree_depth=4, step_size=opt_step)\n",
    "kernel = tfp.mcmc.DualAveragingStepSizeAdaptation(inner_kernel, num_adaptation_steps=int(burn * 0.8))\n",
    "start = time.time()\n",
    "\n",
    "samples, kernel_results = tfp.mcmc.sample_chain(num_results=num_samples, num_burnin_steps=burn, num_steps_between_results=0, current_state=mover.kernel_params, kernel=kernel)\n",
    "end = time.time()\n",
    "print(i,end - start)\n",
    "mover.samples_ = samples\n",
    "print(np.sum(kernel_results.inner_results.is_accepted.numpy()/num_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 108.34\n",
    "2 295\n",
    "3 638\n",
    "4 1322"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_results.new_step_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps = []\n",
    "# for j, param in enumerate(mover.kernel_params):\n",
    "#     print(j,mover.kernel_params[j].shape)\n",
    "#     if j==2:\n",
    "#         steps.append(10*s1)\n",
    "#     elif j==6:\n",
    "#         steps.append(10*s2)\n",
    "#     else:\n",
    "#         steps.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/staff1/ctorney/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow_probability/python/mcmc/sample.py:333: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
      "  warnings.warn(\"Tracing all kernel results by default is deprecated. Set \"\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# 200/200 took 6.31 - run for 1000/1000 = 31.551hrs\n",
    "num_samples=250#00\n",
    "burn_in=100#500  ## 100/250\n",
    "num_runs=1#4\n",
    "rescale = 24\n",
    "\n",
    "for i in range(num_runs):\n",
    "    ss = np.float64(1e-3)\n",
    "    steps = []\n",
    "    for j, param in enumerate(mover.kernel_params):\n",
    "        steps.append(ss)\n",
    "    steps=opt_step\n",
    "    kr = mover.nuts_sample(num_samples=num_samples, skip=0, burn_in=burn_in, max_tree_depth=5, init_step=steps)\n",
    "   \n",
    "    # save the results\n",
    "    final_hmc_np = [j.numpy() for j in  mover.samples_]\n",
    "    with open('data/hmc_samples_p_' + str(i) + '.npy', 'wb') as fp:\n",
    "        pickle.dump(final_hmc_np, fp)\n",
    "        \n",
    "    lengths = mover.get_lengthscale_samples()\n",
    "    amps = mover.get_amplitude_samples()\n",
    "    np.save('data/len_sheep_p_' + str(i) + '.npy',lengths)\n",
    "    np.save('data/amp_sheep_p_' + str(i) + '.npy',amps)\n",
    "    \n",
    "\n",
    "    Zin = np.linspace(0,24,num=200,endpoint=False).astype(np.float64)[:,None]\n",
    "\n",
    "    # sample from the posterior)\n",
    "    lengths = mover.get_lengthscale_samples(X=Zin)\n",
    "    amps = mover.get_amplitude_samples(X=Zin)\n",
    "\n",
    "    np.save('data/full_len_sheep_p_' + str(i) + '.npy',lengths)\n",
    "    np.save('data/full_amp_sheep_p_' + str(i) + '.npy',amps)\n",
    "    np.save('data/Z_pred_p_' + str(i) + '.npy',Zin)\n",
    "\n",
    "    \n",
    "    end = time.time()\n",
    "    print(i,end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kr.new_step_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = time.time()\n",
    "# 200/200 took 6.31 - run for 1000/1000 = 31.551hrs\n",
    "num_samples=1500\n",
    "burn_in=500\n",
    "num_runs=1\n",
    "rescale = 24\n",
    "\n",
    "for i in range(num_runs):\n",
    "    steps = []\n",
    "    \n",
    "    ss = np.float64(1e-1)\n",
    "    \n",
    "    for j, param in enumerate(mover_hmc.kernel_params):\n",
    "        if j==1:\n",
    "            steps.append(s1)\n",
    "        elif j==3:\n",
    "            steps.append(s2)\n",
    "        else:\n",
    "            steps.append(0)\n",
    "    kr = mover_hmc.hmc_sample(num_samples=num_samples, skip=0, burn_in=burn_in, num_leapfrog_steps=5, init_step=steps)\n",
    "   \n",
    "    # save the results\n",
    "    final_hmc_np = [j.numpy() for j in  mover_hmc.samples_]\n",
    "    with open('data/hmc_samples_p_' + str(i) + '.npy', 'wb') as fp:\n",
    "        pickle.dump(final_hmc_np, fp)\n",
    "        \n",
    "    lengths = mover_hmc.get_lengthscale_samples()\n",
    "    amps = mover_hmc.get_amplitude_samples()\n",
    "    np.save('data/len_sheep_p_' + str(i) + '.npy',lengths)\n",
    "    np.save('data/amp_sheep_p_' + str(i) + '.npy',amps)\n",
    "    \n",
    "\n",
    "    Zin = np.linspace(0,24,num=200,endpoint=False).astype(np.float64)[:,None]\n",
    "\n",
    "    # sample from the posterior)\n",
    "    lengths = mover_hmc.get_lengthscale_samples(X=Zin)\n",
    "    amps = mover_hmc.get_amplitude_samples(X=Zin)\n",
    "\n",
    "    np.save('data/full_len_sheep_p_' + str(i) + '.npy',lengths)\n",
    "    np.save('data/full_amp_sheep_p_' + str(i) + '.npy',amps)\n",
    "    np.save('data/Z_pred_p_' + str(i) + '.npy',Zin)\n",
    "\n",
    "    \n",
    "    end = time.time()\n",
    "    print(i,end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('time: ',10*(end - start)/60/60)\n",
    "\n",
    "print(np.sum(kernel_results.inner_results.is_accepted.numpy()/num_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kr.new_step_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "25.4872/5*6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mp in mover.kernel_params:\n",
    "    print(mp)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescale = 24\n",
    "Z_pred = np.arange(0,1.0*rescale,rescale*0.001).astype(np.float64)[:,None]\n",
    "lengths = mover_hmc.get_lengthscale_samples(X=Z_pred)\n",
    "amps = mover_hmc.get_amplitude_samples(X=Z_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Zin = np.linspace(0,24,num=500,endpoint=False).astype(np.float64)[:,None]\n",
    "\n",
    "# sample from the posterior)\n",
    "lengths = mover.get_lengthscale_samples(X=Zin)\n",
    "amps = mover.get_amplitude_samples(X=Zin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  plot the sample for checking\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "\n",
    "Z_time=np.array([np.datetime64('2019') + np.timedelta64(int(k*60*60), 's') for k in Zin])\n",
    "\n",
    "for i in range(0,lengths.shape[0]):\n",
    "    plt.plot(Z_time[np.argsort(Z_time)],lengths[i,np.argsort(Z_time)],c='C1',alpha=0.902,linewidth=1.0)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.xaxis_date()\n",
    "ax.xaxis.set_major_formatter(myFmt)\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "for i in range(0,amps.shape[0]):\n",
    "    plt.plot(Z_time[np.argsort(Z_time)],amps[i,np.argsort(Z_time)],c='C1', alpha=0.902,linewidth=1.0)\n",
    "ax = plt.gca()\n",
    "ax.xaxis_date()\n",
    "ax.xaxis.set_major_formatter(myFmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
